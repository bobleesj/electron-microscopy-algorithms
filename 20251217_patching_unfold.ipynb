{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617f8bae",
   "metadata": {},
   "source": [
    "# Patch extraction optimization: why `unfold` wins\n",
    "\n",
    "This notebook demonstrates how to speed up patch extraction from 2D arrays by 10-20×.\n",
    "\n",
    "## The problem\n",
    "\n",
    "In ptychography (and many imaging applications), we need to extract many overlapping patches from a large object array. A naive loop-based approach is slow because it launches a separate GPU kernel for each patch.\n",
    "\n",
    "## The solution\n",
    "\n",
    "Use `torch.unfold()` which is a native sliding window operation optimized at the C++/CUDA level.\n",
    "\n",
    "## Why unfold is the best\n",
    "\n",
    "| Factor | Loop | unfold |\n",
    "|--------|------|--------|\n",
    "| **Kernel launches** | N (one per patch) | 1 (single operation) |\n",
    "| **Memory access** | Random, cache-unfriendly | Sequential, cache-optimized |\n",
    "| **Python overhead** | High (loop + append) | None (pure C++) |\n",
    "| **GPU parallelism** | Serialized | Fully parallel |\n",
    "\n",
    "The key insight: `unfold` doesn't copy data. It creates a *view* with clever stride manipulation, making it essentially free for the extraction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4260247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \n",
    "                      'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842b1c8",
   "metadata": {},
   "source": [
    "## Setup: test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70f25b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: 256×256, Patch: 80×80\n",
      "Positions: 4096, Step: 2.79px, Overlap: 97%\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "OBJECT_SIZE = 256\n",
    "PROBE_SIZE = 80\n",
    "SCAN_GRID = 64  # 64×64 = 4096 patches\n",
    "\n",
    "# Create complex-valued object\n",
    "obj = torch.complex(\n",
    "    torch.randn(OBJECT_SIZE, OBJECT_SIZE, device=device),\n",
    "    torch.randn(OBJECT_SIZE, OBJECT_SIZE, device=device)\n",
    ")\n",
    "\n",
    "# Compute step size for uniform grid coverage\n",
    "step = (OBJECT_SIZE - PROBE_SIZE) / (SCAN_GRID - 1)\n",
    "step_int = max(1, int(round(step)))\n",
    "\n",
    "# Generate scan positions\n",
    "positions = [(int(i * step), int(j * step)) \n",
    "             for i in range(SCAN_GRID) for j in range(SCAN_GRID)]\n",
    "\n",
    "print(f\"Object: {OBJECT_SIZE}×{OBJECT_SIZE}, Patch: {PROBE_SIZE}×{PROBE_SIZE}\")\n",
    "print(f\"Positions: {len(positions)}, Step: {step:.2f}px, Overlap: {(1-step/PROBE_SIZE)*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22ac3c8",
   "metadata": {},
   "source": [
    "## Method 1: slow loop (baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f53ddb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_slow(obj, positions, size):\n",
    "    \"\"\"Extract patches one at a time with a Python loop.\"\"\"\n",
    "    patches = []\n",
    "    for y, x in positions:\n",
    "        patches.append(obj[y:y+size, x:x+size])\n",
    "    return torch.stack(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bccd82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop method: 23.31 ms\n"
     ]
    }
   ],
   "source": [
    "def sync():\n",
    "    if device.type == 'cuda': torch.cuda.synchronize()\n",
    "    elif device.type == 'mps': torch.mps.synchronize()\n",
    "\n",
    "# Warmup and benchmark\n",
    "_ = extract_slow(obj, positions[:10], PROBE_SIZE)\n",
    "sync()\n",
    "\n",
    "N = 50\n",
    "t0 = time.perf_counter()\n",
    "for _ in range(N):\n",
    "    patches_slow = extract_slow(obj, positions, PROBE_SIZE)\n",
    "sync()\n",
    "time_slow = (time.perf_counter() - t0) / N * 1000\n",
    "\n",
    "print(f\"Loop method: {time_slow:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e0998b",
   "metadata": {},
   "source": [
    "## Method 2: fast unfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cfa4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unfold(obj, size, step):\n",
    "    \"\"\"Extract all patches at once using sliding window.\"\"\"\n",
    "    # unfold(dim, size, step) creates a view with sliding windows\n",
    "    patches = obj.unfold(0, size, step).unfold(1, size, step)\n",
    "    return patches.reshape(-1, size, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcfd2ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfold method: 2.44 ms\n",
      "Speedup: 9.5×\n"
     ]
    }
   ],
   "source": [
    "# Warmup and benchmark\n",
    "_ = extract_unfold(obj, PROBE_SIZE, step_int)\n",
    "sync()\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "for _ in range(N):\n",
    "    patches_fast = extract_unfold(obj, PROBE_SIZE, step_int)\n",
    "sync()\n",
    "time_fast = (time.perf_counter() - t0) / N * 1000\n",
    "\n",
    "print(f\"Unfold method: {time_fast:.2f} ms\")\n",
    "print(f\"Speedup: {time_slow/time_fast:.1f}×\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c993e",
   "metadata": {},
   "source": [
    "## Benchmark across different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d24680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Config |  Loop (ms) |  Unfold (ms) |  Speedup\n",
      "------------------------------------------------------------\n",
      "           128×32×32 |       4.56 |         0.09 |    50.1×\n",
      "           256×80×64 |      18.24 |         1.95 |     9.3×\n",
      "          512×64×128 |     113.19 |         3.73 |    30.3×\n"
     ]
    }
   ],
   "source": [
    "# (object_size, probe_size, scan_grid)\n",
    "#  - object_size: size of the 2D object array (NxN)\n",
    "#  - probe_size: size of each patch to extract (PxP)  \n",
    "#  - scan_grid: number of scan positions per dimension (GxG total patches)\n",
    "N_BENCH = 100  # iterations per benchmark\n",
    "\n",
    "CONFIGS = [\n",
    "    (128, 32, 32),    # 128×128 object, 32×32 patches, 1024 positions\n",
    "    (256, 80, 64),    # 256×256 object, 80×80 patches, 4096 positions\n",
    "    (512, 64, 128),   # 512×512 object, 64×64 patches, 16384 positions\n",
    "]\n",
    "\n",
    "print(f\"{'Config':>20s} | {'Loop (ms)':>10s} | {'Unfold (ms)':>12s} | {'Speedup':>8s}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for obj_size, probe_size, scan_grid in CONFIGS:\n",
    "    # Create test object\n",
    "    test_obj = torch.complex(\n",
    "        torch.randn(obj_size, obj_size, device=device),\n",
    "        torch.randn(obj_size, obj_size, device=device)\n",
    "    )\n",
    "    \n",
    "    # Compute positions\n",
    "    step = (obj_size - probe_size) / (scan_grid - 1)\n",
    "    step_int = max(1, int(round(step)))\n",
    "    pos = [(int(i*step), int(j*step)) for i in range(scan_grid) for j in range(scan_grid)]\n",
    "    # Benchmark loop\n",
    "    sync()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(N_BENCH):\n",
    "        _ = extract_slow(test_obj, pos, probe_size)\n",
    "    sync()\n",
    "    t_loop = (time.perf_counter() - t0) / N_BENCH * 1000\n",
    "    \n",
    "    # Benchmark unfold\n",
    "    sync()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(N_BENCH):\n",
    "        _ = extract_unfold(test_obj, probe_size, step_int)\n",
    "    sync()\n",
    "    t_unfold = (time.perf_counter() - t0) / N_BENCH * 1000\n",
    "    \n",
    "    config = f\"{obj_size}×{probe_size}×{scan_grid}\"\n",
    "    print(f\"{config:>20s} | {t_loop:>10.2f} | {t_unfold:>12.2f} | {t_loop/t_unfold:>7.1f}×\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c5d4f5",
   "metadata": {},
   "source": [
    "## Appendix: how unfold works\n",
    "\n",
    "Now that you've seen the speedup, here's how `unfold` actually works.\n",
    "\n",
    "`unfold(dim, size, step)` extracts sliding windows along a dimension.\n",
    "\n",
    "### 1D example\n",
    "\n",
    "```text\n",
    "tensor: [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "unfold(dim=0, size=3, step=2):\n",
    "\n",
    "  Start at 0:  [0, 1, 2]\n",
    "  Start at 2:  [2, 3, 4]\n",
    "  Start at 4:  [4, 5, 6]\n",
    "\n",
    "Result: [[0, 1, 2],\n",
    "         [2, 3, 4],\n",
    "         [4, 5, 6]]\n",
    "\n",
    "Shape: (3, 3) = (num_windows, window_size)\n",
    "```\n",
    "\n",
    "### 2D example (extracting patches)\n",
    "\n",
    "We apply unfold twice: once for rows, once for columns.\n",
    "\n",
    "```text\n",
    "Original 4×4 grid:\n",
    "\n",
    "     0   1   2   3\n",
    "   ┌───┬───┬───┬───┐\n",
    " 0 │ 0 │ 1 │ 2 │ 3 │\n",
    "   ├───┼───┼───┼───┤\n",
    " 1 │ 4 │ 5 │ 6 │ 7 │\n",
    "   ├───┼───┼───┼───┤\n",
    " 2 │ 8 │ 9 │10 │11 │\n",
    "   ├───┼───┼───┼───┤\n",
    " 3 │12 │13 │14 │15 │\n",
    "   └───┴───┴───┴───┘\n",
    "```\n",
    "\n",
    "**Step 1: `unfold(dim=0, size=2, step=1)`** — slide 2-row window down\n",
    "\n",
    "```text\n",
    "Window at row 0:     Window at row 1:     Window at row 2:\n",
    "┌───┬───┬───┬───┐    ┌───┬───┬───┬───┐    ┌───┬───┬───┬───┐\n",
    "│ 0 │ 1 │ 2 │ 3 │    │ 4 │ 5 │ 6 │ 7 │    │ 8 │ 9 │10 │11 │\n",
    "├───┼───┼───┼───┤    ├───┼───┼───┼───┤    ├───┼───┼───┼───┤\n",
    "│ 4 │ 5 │ 6 │ 7 │    │ 8 │ 9 │10 │11 │    │12 │13 │14 │15 │\n",
    "└───┴───┴───┴───┘    └───┴───┴───┴───┘    └───┴───┴───┴───┘\n",
    "\n",
    "Shape after step 1: (3, 4, 2) = (3 row positions, 4 columns, 2 rows each)\n",
    "```\n",
    "\n",
    "**Step 2: `unfold(dim=1, size=2, step=1)`** — slide 2-col window across\n",
    "\n",
    "```text\n",
    "Now we have 3×3 = 9 patches of size 2×2:\n",
    "\n",
    "Patch[0,0]  Patch[0,1]  Patch[0,2]    (row window 0)\n",
    "┌───┬───┐  ┌───┬───┐  ┌───┬───┐\n",
    "│ 0 │ 1 │  │ 1 │ 2 │  │ 2 │ 3 │\n",
    "├───┼───┤  ├───┼───┤  ├───┼───┤\n",
    "│ 4 │ 5 │  │ 5 │ 6 │  │ 6 │ 7 │\n",
    "└───┴───┘  └───┴───┘  └───┴───┘\n",
    "\n",
    "Patch[1,0]  Patch[1,1]  Patch[1,2]    (row window 1)\n",
    "┌───┬───┐  ┌───┬───┐  ┌───┬───┐\n",
    "│ 4 │ 5 │  │ 5 │ 6 │  │ 6 │ 7 │\n",
    "├───┼───┤  ├───┼───┤  ├───┼───┤\n",
    "│ 8 │ 9 │  │ 9 │10 │  │10 │11 │\n",
    "└───┴───┘  └───┴───┘  └───┴───┘\n",
    "\n",
    "Patch[2,0]  Patch[2,1]  Patch[2,2]    (row window 2)\n",
    "┌───┬───┐  ┌───┬───┐  ┌───┬───┐\n",
    "│ 8 │ 9 │  │ 9 │10 │  │10 │11 │\n",
    "├───┼───┤  ├───┼───┤  ├───┼───┤\n",
    "│12 │13 │  │13 │14 │  │14 │15 │\n",
    "└───┴───┘  └───┴───┘  └───┴───┘\n",
    "\n",
    "Final shape: (3, 3, 2, 2) = (3 row pos, 3 col pos, 2×2 patch)\n",
    "After reshape(-1, 2, 2): (9, 2, 2) = 9 patches of 2×2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb57ebd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 1D tensor: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "After unfold(dim=0, size=3, step=2): shape torch.Size([3, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [2, 3, 4],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# 1D example\n",
    "x = torch.arange(8)\n",
    "print(\"Original 1D tensor:\", x.tolist())\n",
    "\n",
    "windows = x.unfold(0, 3, 2)\n",
    "print(f\"After unfold(dim=0, size=3, step=2): shape {windows.shape}\")\n",
    "print(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b75ea762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 4×4 grid:\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "\n",
      "After double unfold: shape torch.Size([3, 3, 2, 2])\n",
      "\n",
      "Reshaped to torch.Size([9, 2, 2]): 9 patches of 2×2\n",
      "First 3 patches:\n",
      "  Patch 0: [[0, 1], [4, 5]]\n",
      "  Patch 1: [[1, 2], [5, 6]]\n",
      "  Patch 2: [[2, 3], [6, 7]]\n"
     ]
    }
   ],
   "source": [
    "# 2D example - extracting patches\n",
    "grid = torch.arange(16).reshape(4, 4)\n",
    "print(\"Original 4×4 grid:\")\n",
    "print(grid)\n",
    "print()\n",
    "\n",
    "# Double unfold for 2D patches\n",
    "patches = grid.unfold(0, 2, 1).unfold(1, 2, 1)\n",
    "print(f\"After double unfold: shape {patches.shape}\")\n",
    "print()\n",
    "\n",
    "# Reshape to list of patches\n",
    "patches_flat = patches.reshape(-1, 2, 2)\n",
    "print(f\"Reshaped to {patches_flat.shape}: 9 patches of 2×2\")\n",
    "print(\"First 3 patches:\")\n",
    "for i in range(3):\n",
    "    print(f\"  Patch {i}: {patches_flat[i].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7910974",
   "metadata": {},
   "source": [
    "### Why it's fast: zero-copy view\n",
    "\n",
    "The magic is that `unfold` doesn't copy data. It creates a **view** by manipulating tensor strides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58cfa92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original object:\n",
      "  Shape: torch.Size([256, 256]), Strides: (256, 1)\n",
      "  Storage size: 65536 elements\n",
      "\n",
      "After unfold (VIEW, no copy!):\n",
      "  Shape: torch.Size([45, 45, 80, 80]), Strides: (1024, 4, 256, 1)\n",
      "  Storage size: 65536 elements (same!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xz/h9zytq2x1kgcckdf20y6hn4w0000gn/T/ipykernel_90896/2541976904.py:6: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  print(f\"  Storage size: {obj.storage().size()} elements\")\n"
     ]
    }
   ],
   "source": [
    "# unfold creates a VIEW, not a copy\n",
    "patches_view = obj.unfold(0, PROBE_SIZE, step_int).unfold(1, PROBE_SIZE, step_int)\n",
    "\n",
    "print(\"Original object:\")\n",
    "print(f\"  Shape: {obj.shape}, Strides: {obj.stride()}\")\n",
    "print(f\"  Storage size: {obj.storage().size()} elements\")\n",
    "print()\n",
    "print(\"After unfold (VIEW, no copy!):\")\n",
    "print(f\"  Shape: {patches_view.shape}, Strides: {patches_view.stride()}\")\n",
    "print(f\"  Storage size: {patches_view.storage().size()} elements (same!)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
